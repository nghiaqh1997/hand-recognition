{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b84491-f40b-4996-91c1-48eb20223614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sqlite3\n",
    "import imutils\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2650cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gui_py():\n",
    "    button_reset_game = pyautogui.locateOnScreen('img_game/2.bmp')\n",
    "    #print(button_reset_game)\n",
    "    button_reset_game = pyautogui.center(button_reset_game)\n",
    "    #print(button_reset_game)\n",
    "    button_reset_game_x,button_reset_game_y = button_reset_game\n",
    "    #print(button_reset_game_x)\n",
    "    #print(button_reset_game_y)\n",
    "    pyautogui.moveTo(button_reset_game_x,button_reset_game_y,5)\n",
    "    pyautogui.click(button_reset_game_x,button_reset_game_y,5)\n",
    "#gui_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ddabf-9fe7-4e60-9a3c-e88a97623962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_avg(image,aWeight):\n",
    "    global bg\n",
    "    if bg is None :\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return \n",
    "    cv2.accumulateWeighted(image,bg,aWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380d9a8-107a-4744-8304-c18ff3423b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image, threshold=30):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    ret, thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    #ret, thresholded = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ca385-ec74-412c-890d-9b3c46fec22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b748897-3d10-49d7-a778-ce02e2869ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = None\n",
    "model = tf.keras.models.load_model('cnn_model_keras2.h5')\n",
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/0/100.jpg', 0)\n",
    "    return img.shape\n",
    "# 50,50\n",
    "image_x, image_y = get_image_size()\n",
    "def tf_process_image(img):\n",
    "    img = cv2.resize(img, (image_x, image_y))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    np_array = np.array(img)\n",
    "    return np_array\n",
    "# shape 50,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6129b6-9ef4-4d42-831d-ddd10abf6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_process_image(img):\n",
    "    img = cv2.resize(img, (image_x, image_y))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.reshape(img, (1, image_x, image_y, 1))\n",
    "    return img\n",
    "\n",
    "def keras_predict(model, image):\n",
    "    processed = keras_process_image(image)\n",
    "    pred_probab = model.predict(processed)[0]\n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class\n",
    "\n",
    "def get_pred_text_from_db(pred_class):\n",
    "    conn = sqlite3.connect(\"gesture_db.db\")\n",
    "    cmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class)\n",
    "    cursor = conn.execute(cmd)\n",
    "    for row in cursor:\n",
    "        return row[0]\n",
    "\n",
    "def split_sentence(text, num_of_words):\n",
    "    '''\n",
    "    Splits a text into group of num_of_words\n",
    "    '''\n",
    "    list_words = text.split(\" \")\n",
    "    length = len(list_words)\n",
    "    splitted_sentence = []\n",
    "    b_index = 0\n",
    "    e_index = num_of_words\n",
    "    while length > 0:\n",
    "        part = \"\"\n",
    "        for word in list_words[b_index:e_index]:\n",
    "            part = part + \" \" + word\n",
    "        splitted_sentence.append(part)\n",
    "        b_index += num_of_words\n",
    "        e_index += num_of_words\n",
    "        length -= num_of_words\n",
    "    return splitted_sentence\n",
    "\n",
    "def put_splitted_text_in_blackboard(blackboard, splitted_text):\n",
    "    y = 200\n",
    "    for text in splitted_text:\n",
    "        cv2.putText(blackboard, text, (4, y), cv2.FONT_HERSHEY_TRIPLEX, 2, (255, 255, 255))\n",
    "        y += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c51836-ce11-4586-a252-8db6fda387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize():\n",
    "    global prediction\n",
    "    aWeight = 0.5\n",
    "    top, right, bottom, left = 120, 0, 350, 200\n",
    "    #top, right, bottom, left = 10, 350, 225, 590\n",
    "    num_frames = 0\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        text = \"\"\n",
    "        img = cam.read()[1]\n",
    "        img = cv2.flip(img, 1)\n",
    "        img = cv2.resize(img, (640, 480))\n",
    "        clone = img.copy()\n",
    "        roi = img[top:bottom, right:left]\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, aWeight)\n",
    "        else:\n",
    "            # segment the hand region\n",
    "            hand = segment(gray)\n",
    "\n",
    "            # check whether hand region is segmented\n",
    "            if hand is not None:\n",
    "                # if yes, unpack the thresholded image and\n",
    "                # segmented region\n",
    "                (thresholded, segmented) = hand\n",
    "\n",
    "                # draw the segmented region and display the frame\n",
    "                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "                thresh = thresholded\n",
    "                # draw the segmented hand\n",
    "                cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n",
    "                contours = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n",
    "                # increment the number of frames\n",
    "        num_frames += 1\n",
    "        cv2.imshow(\"Video\", clone) \n",
    "        try:\n",
    "            if len(contours) > 0:\n",
    "                contour = max(contours, key = cv2.contourArea)\n",
    "                #print(cv2.contourArea(contour))\n",
    "                if cv2.contourArea(contour) > 10000:\n",
    "                    x1, y1, w1, h1 = cv2.boundingRect(contour)\n",
    "                    save_img = thresh[y1:y1+h1, x1:x1+w1]\n",
    "                    \n",
    "                    if w1 > h1:\n",
    "                        save_img = cv2.copyMakeBorder(save_img, int((w1-h1)/2) , int((w1-h1)/2) , 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                    elif h1 > w1:\n",
    "                        save_img = cv2.copyMakeBorder(save_img, 0, 0, int((h1-w1)/2) , int((h1-w1)/2) , cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "                    \n",
    "                    pred_probab, pred_class = keras_predict(model, save_img)\n",
    "                    \n",
    "                    if pred_probab*100 > 80:\n",
    "                        text = get_pred_text_from_db(pred_class)\n",
    "                        print(text)\n",
    "                    if text == '9':\n",
    "                        print(\"hehe\")\n",
    "                    elif text =='10':\n",
    "                        print(\"hoho\")\n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "            splitted_text = split_sentence(text, 2)\n",
    "            put_splitted_text_in_blackboard(blackboard, splitted_text)\n",
    "            #cv2.putText(blackboard, text, (30, 200), cv2.FONT_HERSHEY_TRIPLEX, 1.3, (255, 255, 255))\n",
    "            res = np.hstack((img, blackboard))\n",
    "            cv2.imshow(\"Recognizing gesture\", res)\n",
    "            cv2.imshow(\"thresh\", thresh)\n",
    "        except:\n",
    "            pass\n",
    "        #cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dd6c5-e2af-4f5b-81f2-b0e49acac46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = None\n",
    "keras_predict(model, np.zeros((50, 50), dtype=np.uint8))\t\t\n",
    "recognize()\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
